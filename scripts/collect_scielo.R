library(rvest)
library(dplyr)
library(stringr)
library(purrr)

# ğŸ”¹ Lista de meses em diferentes idiomas para remoÃ§Ã£o do tÃ­tulo
meses_lista <- c(
  "Jan", "Fev", "Mar", "Abr", "Mai", "Jun", "Jul", "Ago", "Set", "Out", "Nov", "Dez",
  "January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December",
  "Enero", "Febrero", "Marzo", "Abril", "Mayo", "Junio", "Julio", "Agosto", "Septiembre", "Octubre", "Noviembre", "Diciembre")

# ğŸ”¹ FunÃ§Ã£o para buscar artigos no SciELO
buscar_scielo <- function(query, num_paginas = 8) {

  base_url <- "https://search.scielo.org/?q="
  encoded_query <- URLencode(query)  # Codificar a query para URL
  all_articles <- list()

  for (page in 1:num_paginas) {
    message(paste("ğŸ” Buscando pÃ¡gina", page, "..."))

    # Construir URL correta com `&from=` para evitar pular artigos
    from_value <- (page - 1) * 15 + 1  # Define o inÃ­cio dos artigos por pÃ¡gina
    search_url <- paste0(base_url, encoded_query, "&lang=pt&count=15&from=", from_value, "&page=", page)

    # Ler a pÃ¡gina HTML
    pagina <- tryCatch(read_html(search_url), error = function(e) return(NULL))
    if (is.null(pagina)) {
      message(paste("âŒ Falha ao carregar a pÃ¡gina", page))
      next
    }

    # ğŸ”¹ Extrair os blocos de artigos
    artigos <- pagina %>% html_nodes(".item")

    # ğŸ”¹ Criar listas para armazenar os dados
    ids <- artigos %>% html_attr("id")  # Pega o ID do artigo

    titulos <- artigos %>% html_nodes(".title") %>% html_text(trim = TRUE)

    # Capturar mÃºltiplos autores corretamente
    autores <- artigos %>% map_chr(~{
      autores_lista <- .x %>% html_nodes(".line.authors .author") %>% html_text(trim = TRUE)
      if (length(autores_lista) == 0) return(NA) else return(paste(autores_lista, collapse = ", "))
    })

    # ğŸ”¹ Capturar e limpar corretamente as informaÃ§Ãµes do periÃ³dico
    revista_info <- artigos %>% map_chr(~{
      info <- .x %>% html_nodes(".line.source") %>% html_text(trim = TRUE)
      if (length(info) == 0) return(NA) else return(info[1])
    }) %>%
      str_squish() %>%  # Remove espaÃ§os extras
      str_remove_all("MÃ©tricas do periÃ³dico|Sobre o periÃ³dico|SciELO Analytics") %>%  # Remove textos indesejados
      str_squish()  # Remove espaÃ§os extras restantes apÃ³s remoÃ§Ã£o

    # ğŸ”¹ Separar `journal_title` e `year`, removendo os meses do `journal_title`
    journal_titles <- str_extract(revista_info, "^[^0-9]+") %>%
      str_trim() %>%
      str_remove_all(paste0("\\b(", paste(meses_lista, collapse = "|"), ")\\b")) %>%  # Remove meses
      str_squish()  # Remover espaÃ§os extras

    years <- str_extract(revista_info, "\\b\\d{4}\\b")  # Extrai o ano de publicaÃ§Ã£o

    # ğŸ”¹ Capturar corretamente o resumo, verificando se estÃ¡ escondido
    resumos <- artigos %>% map_chr(~{
      resumo_full <- .x %>% html_nodes(".abstract") %>% html_text(trim = TRUE)

      # Se o resumo estiver ausente, verificar se hÃ¡ um link de resumo oculto
      if (length(resumo_full) == 0) {
        resumo_hidden <- .x %>% html_nodes(".abstract[style='display:none']") %>% html_text(trim = TRUE)
        if (length(resumo_hidden) > 0) {
          return(resumo_hidden)  # Retorna o resumo oculto
        }
        return(NA)  # Se ainda nÃ£o houver resumo, retorna NA
      }

      return(paste(resumo_full, collapse = " "))
    })

    # Capturar o DOI corretamente
    dois <- artigos %>% map_chr(~{
      doi_link <- .x %>% html_nodes(".DOIResults a") %>% html_attr("href")
      if (length(doi_link) == 0) return(NA) else return(doi_link[1])
    })

    # ğŸ”¹ Garantir que todas as colunas tenham o mesmo nÃºmero de elementos
    max_len <- max(length(ids), length(titulos), length(autores), length(revista_info), length(journal_titles), length(years), length(resumos), length(dois))

    ids <- c(ids, rep(NA, max_len - length(ids)))
    titulos <- c(titulos, rep(NA, max_len - length(titulos)))
    autores <- c(autores, rep(NA, max_len - length(autores)))
    revista_info <- c(revista_info, rep(NA, max_len - length(revista_info)))
    journal_titles <- c(journal_titles, rep(NA, max_len - length(journal_titles)))
    years <- c(years, rep(NA, max_len - length(years)))
    resumos <- c(resumos, rep(NA, max_len - length(resumos)))
    dois <- c(dois, rep(NA, max_len - length(dois)))

    # Criar um DataFrame com os dados coletados
    df <- tibble(
      article_id = ids,
      title = titulos,
      authors = autores,
      journal_info = revista_info,  # ğŸ”¹ MantÃ©m a info original do periÃ³dico com o mÃªs
      journal_title = journal_titles,  # ğŸ”¹ Nome do periÃ³dico limpo (sem meses)
      year = years,  # ğŸ”¹ Ano do artigo
      abstract = resumos,
      doi = dois,
      page_number = page  # ğŸ”¹ NÃºmero da pÃ¡gina original
    )

    # Adicionar Ã  lista de resultados
    all_articles[[page]] <- df
    Sys.sleep(2)  # ğŸ”¹ Pequeno delay para evitar bloqueios
  }

  # Combinar todas as pÃ¡ginas em um Ãºnico DataFrame
  final_df <- bind_rows(all_articles) %>% distinct()  # ğŸ”¹ Remove duplicatas

  return(final_df)
}

# ğŸ”¹ Rodar busca no SciELO
query <- '"data science team" OR "data-science" OR "data science"'

query <- '"origin-destination matrix"'

df_scielo <- buscar_scielo(query, num_paginas = 3)
